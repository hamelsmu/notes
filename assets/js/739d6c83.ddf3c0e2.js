"use strict";(self.webpackChunknotes=self.webpackChunknotes||[]).push([[4426],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),u=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=u(n),m=o,h=d["".concat(l,".").concat(m)]||d[m]||c[m]||r;return n?a.createElement(h,s(s({ref:t},p),{},{components:n})):a.createElement(h,s({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,s=new Array(r);s[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var u=2;u<r;u++)s[u]=n[u];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9870:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return l},metadata:function(){return u},assets:function(){return p},toc:function(){return c},default:function(){return m}});var a=n(7462),o=n(3366),r=(n(7294),n(3905)),s=["components"],i={},l=void 0,u={unversionedId:"k8s/Storage - Basics",id:"k8s/Storage - Basics",title:"Storage - Basics",description:"[[k8s]]",source:"@site/docs/k8s/04-Storage - Basics.md",sourceDirName:"k8s",slug:"/k8s/Storage - Basics",permalink:"/k8s/Storage - Basics",editUrl:"https://github.dev/hamelsmu/notes/blob/master/docs/k8s/04-Storage - Basics.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Secrets",permalink:"/k8s/Secrets"},next:{title:"Storage - Dynamic Provisioning",permalink:"/k8s/Storage - Dynamic Provisioning"}},p={},c=[{value:"Pod Storage",id:"pod-storage",level:2},{value:"HostPath",id:"hostpath",level:2},{value:"Persistent Volumes and Claims",id:"persistent-volumes-and-claims",level:2},{value:"Node Labeling",id:"node-labeling",level:3},{value:"Manual Provisioning",id:"manual-provisioning",level:3},{value:"Binding To the PVC",id:"binding-to-the-pvc",level:3}],d={toc:c};function m(e){var t=e.components,n=(0,o.Z)(e,s);return(0,r.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"[","[k8s]","]"),(0,r.kt)("p",null,"This is Chapter 5 in KIAMOL"),(0,r.kt)("p",null,"Unlike compute, storage is more complicated because you don't want your data to get lost on pod restarts. "),(0,r.kt)("p",null,"Solution: you want to mount external file systems that will survive a container restart.  "),(0,r.kt)("p",null,"ConfigMaps and Secrets are mounted, but those are read aonly. "),(0,r.kt)("h2",{id:"pod-storage"},"Pod Storage"),(0,r.kt)("p",null,"This kind of storage lives outside the container but on the Pod.  It will survive container restarts, but not a Pod restart.  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"%cat sleep/sleep-with-emptyDir.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          volumeMounts:             # Mounts a volume call data\n            - name: data\n              mountPath: /data      # into the /data directory\n      volumes:\n        - name: data           # this is the data volume spec\n          emptyDir: {}         # this is the EmptyDir type\n")),(0,r.kt)("p",null,"If you want your data to persist across pod restarts, you have to mount a different type of storage."),(0,r.kt)("h2",{id:"hostpath"},"HostPath"),(0,r.kt)("p",null,"Writes files to a disk on a node.  So it will survive pod replacements.  However, it is only on that Node and K8s doesn't replicate files to other nodes for you.  ",(0,r.kt)("strong",{parentName:"p"},"Assumes that the replacement pod will always run on the same node :/")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'% cat pi/nginx-with-hostPath.yaml                                                                                                     \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pi-proxy\n  labels:\n    app: pi-proxy\n...\n          volumeMounts:\n            - name: config\n              mountPath: "/etc/nginx/"\n              readOnly: true\n            - name: cache-volume\n              mountPath: /data/nginx/cache\n      volumes:\n        - name: config\n          configMap:\n            name: pi-proxy-configmap\n        - name: cache-volume\n          hostPath:\n            path: /volumes/nginx/cache  #uses a directory non the node\n            type: DirectoryOrCreate #creates a path if it doesn\'t exist\n')),(0,r.kt)("p",null,"[","[HostPath]","] is only a good idea when your app needs temporary storage, because it can dissapear with a node.  You could use Pod Storage for this, too so its not clear when this is useful.  "),(0,r.kt)("h2",{id:"persistent-volumes-and-claims"},"Persistent Volumes and Claims"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"[!Note]","\nThis section is largely pedagoical, you will want to use Dynamic volume provisioning in most cases.")),(0,r.kt)("p",null,"You have to configure shared storage on your cloud provider.  For example, if you had a NFS server with the domain name ",(0,r.kt)("inlineCode",{parentName:"p"},"nfs.my.network")," your PV resource would look like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'% cat todo-list/persistentVolume-nfs.yaml                                                                                            \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv01\nspec:\n  capacity:\n    storage: 50Mi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    server: nfs.my.network\n    path: "/kubernetes-volumes\n')),(0,r.kt)("h3",{id:"node-labeling"},"Node Labeling"),(0,r.kt)("p",null,"If you can use a local storage for a PV like this:"),(0,r.kt)("p",null,"1st make sure your node is labeled: ",(0,r.kt)("inlineCode",{parentName:"p"},"kl label node docker-desktop kiamol=ch05")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"% cat todo-list/persistentVolume.yaml                                                                                                 \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv01\nspec:\n  capacity:\n    storage: 50Mi\n  accessModes:\n    - ReadWriteOnce   # Means that we can only mount this to ONLY ONE POD\n  local:\n    path: /volumes/pv01  # this path must be present on the node\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n        - matchExpressions:\n          - key: kiamol\n            operator: In\n            values:\n              - ch05\n")),(0,r.kt)("p",null,"Pods cannot use this directly, they need to use a [","[PersistenVolumeClaim]","] or PVC.  The PVC gets matched to a PV by K8s which leaves the underling volume details to the Pv. "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'%cat todo-list/postgres-persistentVolumeClaim.yaml                                                                                   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 40Mi                  # 40 MB\n  storageClassName: ""               # A blank name means a PV needs to exist\n')),(0,r.kt)("p",null,"PV is like creating storage\nPVC is requesting storage that Pods use"),(0,r.kt)("h3",{id:"manual-provisioning"},"Manual Provisioning"),(0,r.kt)("p",null,"We have been manually provisioning PV + PVCs "),(0,r.kt)("p",null,"When you ",(0,r.kt)("inlineCode",{parentName:"p"},"kl apply")," the PVC, it will find unbound PVs and then bind them.  "),(0,r.kt)("p",null,"when you run ",(0,r.kt)("inlineCode",{parentName:"p"},"kl get pv")," you will see if the PV is unclaimed yet or not"),(0,r.kt)("p",null,"if you create a PVC that requests more than any PV, it will show a pending status instead of Bound.  "),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"[!Warning]","\nIf you try to deploy a pod that uses an unbound PVC, the Pod will stay in a Pending state until the PVC gets bound")),(0,r.kt)("h3",{id:"binding-to-the-pvc"},"Binding To the PVC"),(0,r.kt)("p",null,"The deployment references the PVC like so:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'% cat todo-list/postgres/todo-db.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-db\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  template:\n    metadata:\n      labels:\n        app: todo-db\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n          env:\n          - name: POSTGRES_PASSWORD_FILE\n            value: /secrets/postgres_password\n          volumeMounts:\n            - name: secret\n              mountPath: "/secrets"\n            - name: data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: secret\n          secret:\n            secretName: todo-db-secret\n            defaultMode: 0400\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\n        - name: data\n          persistentVolumeClaim:\n            claimName: postgres-pvc\n')),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"[!Note]","\nIn production, you want to replace the local volume PV with a distributed volume supported by your cloud provider or cluster.  "),(0,r.kt)("p",{parentName:"blockquote"},"The PVC doesn't care about the implementation so you will just have to swap out the PV")))}m.isMDXComponent=!0}}]);