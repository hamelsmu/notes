"use strict";(self.webpackChunknotes=self.webpackChunknotes||[]).push([[100],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),d=c(a),m=i,h=d["".concat(s,".").concat(m)]||d[m]||u[m]||r;return a?n.createElement(h,l(l({ref:t},p),{},{components:a})):n.createElement(h,l({ref:t},p))}));function h(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,l=new Array(r);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[d]="string"==typeof e?e:i,l[1]=o;for(var c=2;c<r;c++)l[c]=a[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},3901:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(7294),i=a(3743);const r="tableOfContentsInline_prmo";function l(e){let{toc:t,minHeadingLevel:a,maxHeadingLevel:l}=e;return n.createElement("div",{className:r},n.createElement(i.Z,{toc:t,minHeadingLevel:a,maxHeadingLevel:l,className:"table-of-contents",linkClassName:null}))}},3743:(e,t,a)=>{a.d(t,{Z:()=>h});var n=a(7462),i=a(7294),r=a(6668);function l(e){const t=e.map((e=>({...e,parentIndex:-1,children:[]}))),a=Array(7).fill(-1);t.forEach(((e,t)=>{const n=a.slice(2,e.level);e.parentIndex=Math.max(...n),a[e.level]=t}));const n=[];return t.forEach((e=>{const{parentIndex:a,...i}=e;a>=0?t[a].children.push(i):n.push(i)})),n}function o(e){let{toc:t,minHeadingLevel:a,maxHeadingLevel:n}=e;return t.flatMap((e=>{const t=o({toc:e.children,minHeadingLevel:a,maxHeadingLevel:n});return function(e){return e.level>=a&&e.level<=n}(e)?[{...e,children:t}]:t}))}function s(e){const t=e.getBoundingClientRect();return t.top===t.bottom?s(e.parentNode):t}function c(e,t){let{anchorTopOffset:a}=t;const n=e.find((e=>s(e).top>=a));if(n){return function(e){return e.top>0&&e.bottom<window.innerHeight/2}(s(n))?n:e[e.indexOf(n)-1]??null}return e[e.length-1]??null}function p(){const e=(0,i.useRef)(0),{navbar:{hideOnScroll:t}}=(0,r.L)();return(0,i.useEffect)((()=>{e.current=t?0:document.querySelector(".navbar").clientHeight}),[t]),e}function d(e){const t=(0,i.useRef)(void 0),a=p();(0,i.useEffect)((()=>{if(!e)return()=>{};const{linkClassName:n,linkActiveClassName:i,minHeadingLevel:r,maxHeadingLevel:l}=e;function o(){const e=function(e){return Array.from(document.getElementsByClassName(e))}(n),o=function(e){let{minHeadingLevel:t,maxHeadingLevel:a}=e;const n=[];for(let i=t;i<=a;i+=1)n.push(`h${i}.anchor`);return Array.from(document.querySelectorAll(n.join()))}({minHeadingLevel:r,maxHeadingLevel:l}),s=c(o,{anchorTopOffset:a.current}),p=e.find((e=>s&&s.id===function(e){return decodeURIComponent(e.href.substring(e.href.indexOf("#")+1))}(e)));e.forEach((e=>{!function(e,a){a?(t.current&&t.current!==e&&t.current.classList.remove(i),e.classList.add(i),t.current=e):e.classList.remove(i)}(e,e===p)}))}return document.addEventListener("scroll",o),document.addEventListener("resize",o),o(),()=>{document.removeEventListener("scroll",o),document.removeEventListener("resize",o)}}),[e,a])}function u(e){let{toc:t,className:a,linkClassName:n,isChild:r}=e;return t.length?i.createElement("ul",{className:r?void 0:a},t.map((e=>i.createElement("li",{key:e.id},i.createElement("a",{href:`#${e.id}`,className:n??void 0,dangerouslySetInnerHTML:{__html:e.value}}),i.createElement(u,{isChild:!0,toc:e.children,className:a,linkClassName:n}))))):null}const m=i.memo(u);function h(e){let{toc:t,className:a="table-of-contents table-of-contents__left-border",linkClassName:s="table-of-contents__link",linkActiveClassName:c,minHeadingLevel:p,maxHeadingLevel:u,...h}=e;const g=(0,r.L)(),f=p??g.tableOfContents.minHeadingLevel,k=u??g.tableOfContents.maxHeadingLevel,y=function(e){let{toc:t,minHeadingLevel:a,maxHeadingLevel:n}=e;return(0,i.useMemo)((()=>o({toc:l(t),minHeadingLevel:a,maxHeadingLevel:n})),[t,a,n])}({toc:t,minHeadingLevel:f,maxHeadingLevel:k});return d((0,i.useMemo)((()=>{if(s&&c)return{linkClassName:s,linkActiveClassName:c,minHeadingLevel:f,maxHeadingLevel:k}}),[s,c,f,k])),i.createElement(m,(0,n.Z)({toc:y,className:a,linkClassName:s},h))}},263:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>p});var n=a(7462),i=(a(7294),a(3905)),r=a(3901);const l={title:"2_Computer_Vision"},o="Image Classification",s={unversionedId:"fastai/cv",id:"fastai/cv",title:"2_Computer_Vision",description:"Data Prep",source:"@site/docs/fastai/02_cv.md",sourceDirName:"fastai",slug:"/fastai/cv",permalink:"/fastai/cv",draft:!1,editUrl:"https://github.dev/hamelsmu/notes/blob/master/docs/fastai/02_cv.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"2_Computer_Vision"},sidebar:"tutorialSidebar",previous:{title:"1_Fundamentals",permalink:"/fastai/fundamentals"},next:{title:"Batch Predictions",permalink:"/fastai/batch_predicitions"}},c={},p=[{value:"Data Prep",id:"data-prep",level:2},{value:"Debugging",id:"debugging",level:3},{value:"Example of an error in data prep",id:"example-of-an-error-in-data-prep",level:3},{value:"Interpretation",id:"interpretation",level:2},{value:"Improving the model",id:"improving-the-model",level:2},{value:"Learning Rate Finder",id:"learning-rate-finder",level:3},{value:"Fine Tuning models",id:"fine-tuning-models",level:3},{value:"Discriminative Learning Rates",id:"discriminative-learning-rates",level:3},{value:"Mixed Precision Training",id:"mixed-precision-training",level:2},{value:"DataBlock API: Multi-Label Data",id:"datablock-api-multi-label-data",level:2},{value:"Creating Datsets",id:"creating-datsets",level:3},{value:"Using Blocks For Transforms",id:"using-blocks-for-transforms",level:3},{value:"Inspecting Vocabulary",id:"inspecting-vocabulary",level:3},{value:"Using a splitter",id:"using-a-splitter",level:3},{value:"Creating DataLoaders",id:"creating-dataloaders",level:3},{value:"Multi-Label Model",id:"multi-label-model",level:2},{value:"Loss Functions",id:"loss-functions",level:3},{value:"Metrics",id:"metrics",level:3},{value:"Choosing A Prediction Threshold",id:"choosing-a-prediction-threshold",level:3},{value:"Image Regression",id:"image-regression",level:2},{value:"Get Data",id:"get-data",level:3},{value:"Define the DataBlock",id:"define-the-datablock",level:3},{value:"Debug the DataBlock",id:"debug-the-datablock",level:3},{value:"Train The Model",id:"train-the-model",level:3},{value:"Setting <code>y_range</code>",id:"setting-y_range",level:4},{value:"Find the learning rate and then train",id:"find-the-learning-rate-and-then-train",level:4},{value:"Inspect the Results",id:"inspect-the-results",level:4},{value:"Loss Functions",id:"loss-functions-1",level:2}],d={toc:p};function u(e){let{components:t,...l}=e;return(0,i.kt)("wrapper",(0,n.Z)({},d,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"image-classification"},"Image Classification"),(0,i.kt)(r.Z,{toc:p,mdxType:"TOCInline"}),(0,i.kt)("h2",{id:"data-prep"},"Data Prep"),(0,i.kt)("p",null,"Remember, ",(0,i.kt)("inlineCode",{parentName:"p"},"Datablock")," helps create ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders"),".  "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n")),(0,i.kt)("h3",{id:"debugging"},"Debugging"),(0,i.kt)("p",null,"You can debug the Datablock by calling .summary(), which will show you if you have any errors."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'pets.summary(path/"images")\n')),(0,i.kt)("p",null,"If everything looks good, you can use the DataBlock to create a ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders")," instance:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'dls = pets.dataloaders(path/"images")\n')),(0,i.kt)("p",null,"Once you have a ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders")," instance, it is a good idea to call ",(0,i.kt)("inlineCode",{parentName:"p"},"show_batch")," to spot check that things look reasonable:"),(0,i.kt)("p",null,"You can debug this by using ",(0,i.kt)("inlineCode",{parentName:"p"},"show_batch"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> dls.show_batch(nrows=1, ncols=3)\n... [shows images]\n")),(0,i.kt)("p",null,"Finally, you can see what a batch looks like by calling ",(0,i.kt)("inlineCode",{parentName:"p"},"dls.one_batch()")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"x,y = dls.one_batch()\n")),(0,i.kt)("p",null,"You always want to train a model ASAP as your final debugging step.  If you wait too long, you will not discover problems"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n")),(0,i.kt)("h3",{id:"example-of-an-error-in-data-prep"},"Example of an error in data prep"),(0,i.kt)("p",null,"A common error is forgetting to use ",(0,i.kt)("inlineCode",{parentName:"p"},"Resize")," in your DataBlock as an item transform.  For example, the below code will cause an error:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 #forgot to pass `item_tfms=Resize(...),`\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\n")),(0,i.kt)("p",null,"This will complain that it is not able to collate the images because they are of different sizes."),(0,i.kt)("h2",{id:"interpretation"},"Interpretation"),(0,i.kt)("p",null,"You can get diagnostics for your model using this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n")),(0,i.kt)("p",null,'Which will return a confusion matrix.  You can see the "most confused" items by doing this:'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> interp.most_confused(min_val=5)\n[('Bengal', 'Egyptian_Mau', 10),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 8),\n ('Ragdoll', 'Birman', 7),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 6),\n ('american_pit_bull_terrier', 'american_bulldog', 5)]\n")),(0,i.kt)("h2",{id:"improving-the-model"},"Improving the model"),(0,i.kt)("h3",{id:"learning-rate-finder"},"Learning Rate Finder"),(0,i.kt)("p",null,"Start with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)"),(0,i.kt)("li",{parentName:"ul"},"The last point where the loss was clearly decreasing ")),(0,i.kt)("p",null,"The learning rate finder computes those points and more on the curve to help you. Additional learning rate suggestion algorithms can be passed into the function, by default only the valley paradigm is used. The learning rate finder can be called with ",(0,i.kt)("inlineCode",{parentName:"p"},"learn.lr_find"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> learn = cnn_learner(dls, resnet34, metrics=error_rate)\n>>> lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n")),(0,i.kt)("p",null,(0,i.kt)("img",{src:a(9314).Z,width:"1468",height:"1014"})),(0,i.kt)("p",null,"The default ",(0,i.kt)("inlineCode",{parentName:"p"},"valley")," hueristic works just fine.  Note, you will want to re-run this anytime you change your model such as unfreeze layers.  You might want to run this periodically if you are checkpointing during training."),(0,i.kt)("h3",{id:"fine-tuning-models"},"Fine Tuning models"),(0,i.kt)("p",null,"When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the ",(0,i.kt)("inlineCode",{parentName:"p"},"fine_tune")," method fastai does two things:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Trains the randomly added layers for one epoch, with all other layers frozen"),(0,i.kt)("li",{parentName:"ul"},"Unfreezes all of the layers, and trains them all for the number of epochs requested")),(0,i.kt)("p",null,"Although this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The ",(0,i.kt)("inlineCode",{parentName:"p"},"fine_tune")," method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"fit_one_cycle")," is the suggested way to train models without using ",(0,i.kt)("inlineCode",{parentName:"p"},"fine_tune"),". We'll see why later in the book; in short, what ",(0,i.kt)("inlineCode",{parentName:"p"},"fit_one_cycle")," does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3) # train the head\nlearn.unfreeze() # unfreeze everything\nlearn.lr_find() # find new lr after unfreezing\nlearn.fit_one_cycle(6, lr_max=1e-5) #fine tune it all\n")),(0,i.kt)("h3",{id:"discriminative-learning-rates"},"Discriminative Learning Rates"),(0,i.kt)("p",null,"One important aspect of fine tuning is discriminative learning rates: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers)."),(0,i.kt)("p",null,"fastai lets you pass a Python ",(0,i.kt)("inlineCode",{parentName:"p"},"slice")," object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let's use this approach to replicate the previous training, but this time we'll only set the ",(0,i.kt)("em",{parentName:"p"},"lowest")," layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let's train for a while and see what happens:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\n")),(0,i.kt)("p",null,"We can accomplish everything we did above by calling ",(0,i.kt)("inlineCode",{parentName:"p"},"fine_tune")," instead.  ",(0,i.kt)("inlineCode",{parentName:"p"},"fine_tune")," will automatically apply discriminative learning rates for you:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'>>> learn.fine_tune??\nSignature:\nlearn.fine_tune(\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n)\nSource:   \n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    "Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR."\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nFile:      ~/anaconda3/lib/python3.9/site-packages/fastai/callback/schedule.py\nType:      method\n')),(0,i.kt)("h2",{id:"mixed-precision-training"},"Mixed Precision Training"),(0,i.kt)("p",null,"You can achieve mixed precision training to speed up training and give you more memory headroom for bigger models with ",(0,i.kt)("inlineCode",{parentName:"p"},"to_fp16()")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from fastai.callback.fp16 import *\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n")),(0,i.kt)("p",null,"Note how you can use the ",(0,i.kt)("inlineCode",{parentName:"p"},"freeze_epochs")," parameter to keep the base frozen for longer."),(0,i.kt)("h2",{id:"datablock-api-multi-label-data"},"DataBlock API: Multi-Label Data"),(0,i.kt)("p",null,"Let's say you have a Dataframe with filenames and multiple labels per filename.  The best way to get started in to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"DataBlock")," api to construct ",(0,i.kt)("inlineCode",{parentName:"p"},"Datasets")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders"),".  A review of terminology:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Dataset"),": collection that returns a tuple of (x,y) for single item.  Can do this with ",(0,i.kt)("inlineCode",{parentName:"li"},"list(zip(x,y))")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"DataLoader"),": an iterator that provides a stream of minibatches of (x,y) instead of a single item."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Datasets"),": object that contains a training ",(0,i.kt)("inlineCode",{parentName:"li"},"Dataset")," and a ",(0,i.kt)("inlineCode",{parentName:"li"},"Validation")," dataset."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"DataLoaders"),": object that contains a training ",(0,i.kt)("inlineCode",{parentName:"li"},"DataLoader")," and a validation ",(0,i.kt)("inlineCode",{parentName:"li"},"DataLoader"),".")),(0,i.kt)("h3",{id:"creating-datsets"},"Creating Datsets"),(0,i.kt)("p",null,"You can use a ",(0,i.kt)("inlineCode",{parentName:"p"},"DataBlock"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> from fastbook import *\n>>> from fastai.vision.all import *\n\n>>> path = untar_data(URLs.PASCAL_2007)\n>>> df = pd.read_csv(path/'train.csv')\n>>> def get_x(r): return path/'train'/r['fname']\n>>> def get_y(r): return r['labels'].split(' ')\n\n>>> dblock = DataBlock(get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(Path('/home/hamel/.fastai/data/pascal_2007/train/006162.jpg'), ['aeroplane'])\n")),(0,i.kt)("p",null,"Next we need to convert our images into tensors.  We can do this by using the ",(0,i.kt)("inlineCode",{parentName:"p"},"ImageBlock")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"MultiCategoryBlock"),":"),(0,i.kt)("h3",{id:"using-blocks-for-transforms"},"Using Blocks For Transforms"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                       get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n")),(0,i.kt)("h3",{id:"inspecting-vocabulary"},"Inspecting Vocabulary"),(0,i.kt)("p",null,"You can inspect the vocabulary with the ",(0,i.kt)("inlineCode",{parentName:"p"},"vocab")," attribute:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dsets.train.vocab\n")),(0,i.kt)("h3",{id:"using-a-splitter"},"Using a splitter"),(0,i.kt)("p",null,"The dataframe has a column called ",(0,i.kt)("inlineCode",{parentName:"p"},"is_valid"),", we can use that do a train validation split.  By default, the ",(0,i.kt)("inlineCode",{parentName:"p"},"DataBlock")," uses a ",(0,i.kt)("inlineCode",{parentName:"p"},"RandomSplitter"),".  By default, ",(0,i.kt)("inlineCode",{parentName:"p"},"RandomSplitter")," uses 20% of the data for the validation set."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\n")),(0,i.kt)("h3",{id:"creating-dataloaders"},"Creating DataLoaders"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders")," build upon ",(0,i.kt)("inlineCode",{parentName:"p"},"Datasets")," by streaming mini-batches instead of one example at a time. One prerequisite to making ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders")," is that all the images are the same size.  To do this you can use ",(0,i.kt)("inlineCode",{parentName:"p"},"RandomResizedCrop"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n")),(0,i.kt)("p",null,"When you are done with this, you want to debug things by calling ",(0,i.kt)("inlineCode",{parentName:"p"},"show_batch"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dls.show_batch(nrows=1, ncols=3)\n")),(0,i.kt)("h2",{id:"multi-label-model"},"Multi-Label Model"),(0,i.kt)("p",null,"You can create a learner like so:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn = cnn_learner(dls, resnet18)\n")),(0,i.kt)("p",null,"One useful thing is to debug / verify that the output shape conforms to what you are expecting.  You can do this by running a tensor through your model and inspecting it's output:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"x,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n")),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"This is what you would use to extract embeddings / activations out of another model")),(0,i.kt)("p",null,"It's a good idea to see what the activations look like:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> activs[0]\nTensorBase([ 2.0858,  2.8195,  0.0460,  1.7563,  3.3371,  2.4251,  2.3295, -2.8101,  3.3967, -3.2120,  3.3452, -2.3762, -0.3137, -4.6004,  0.7441, -2.6875,  0.0873, -0.2247, -3.1242,  3.6477],\n       grad_fn=<AliasBackward0>)\n")),(0,i.kt)("p",null,"We can see these are not b/w 0 and 1, because the sigmoid has not been applied yet."),(0,i.kt)("h3",{id:"loss-functions"},"Loss Functions"),(0,i.kt)("p",null,"PyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names!"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"F.binary_cross_entropy")," and its module equivalent ",(0,i.kt)("inlineCode",{parentName:"p"},"nn.BCELoss")," calculate cross-entropy on a one-hot-encoded target, but do not include the initial ",(0,i.kt)("inlineCode",{parentName:"p"},"sigmoid"),". Normally for one-hot-encoded targets you'll want ",(0,i.kt)("inlineCode",{parentName:"p"},"F.binary_cross_entropy_with_logits")," (or ",(0,i.kt)("inlineCode",{parentName:"p"},"nn.BCEWithLogitsLoss"),"), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example."),(0,i.kt)("p",null,"The equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is ",(0,i.kt)("inlineCode",{parentName:"p"},"F.nll_loss")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"nn.NLLLoss")," for the version without the initial softmax, and ",(0,i.kt)("inlineCode",{parentName:"p"},"F.cross_entropy")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"nn.CrossEntropyLoss")," for the version with the initial softmax."),(0,i.kt)("p",null,"Since we have a one-hot-encoded target, we will use ",(0,i.kt)("inlineCode",{parentName:"p"},"BCEWithLogitsLoss"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"loss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\n")),(0,i.kt)("p",null,"We don't actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoaders")," has multiple category labels, so it will use ",(0,i.kt)("inlineCode",{parentName:"p"},"nn.BCEWithLogitsLoss")," by default."),(0,i.kt)("h3",{id:"metrics"},"Metrics"),(0,i.kt)("p",null,"We need to make sure we have a metric that works for multi-label classfication:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    "Compute accuracy when `inp` and `targ` are the same size."\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\n')),(0,i.kt)("p",null,"We can use ",(0,i.kt)("inlineCode",{parentName:"p"},"partial")," to set the parameters we want in the metrics function and pass it like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n")),(0,i.kt)("p",null,"You can change your metrics anytime and recalculate things.  ",(0,i.kt)("inlineCode",{parentName:"p"},"validate()")," will return the validation loss and metrics."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> learn.metrics = partial(accuracy_multi, thresh=0.1)\n>>> learn.validate() # returns validation loss and metrics\n(#2) [0.10417556017637253,0.9376891851425171]\n")),(0,i.kt)("p",null,"You can debug metrics by getting the predictions on the validation set with ",(0,i.kt)("inlineCode",{parentName:"p"},"get_preds"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"preds,targs = learn.get_preds()\nassert preds.shape[0] == dls.valid.n\n")),(0,i.kt)("p",null,"Once you have the predictions, you can run the metric function seperately:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"accuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n")),(0,i.kt)("h3",{id:"choosing-a-prediction-threshold"},"Choosing A Prediction Threshold"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"xs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n")),(0,i.kt)("p",null,(0,i.kt)("img",{src:a(8876).Z,width:"898",height:"526"})),(0,i.kt)("p",null,"In this case, we're using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be ",(0,i.kt)("em",{parentName:"p"},"overfitting")," to the validation set, since we're trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we're clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don't try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it's fine to do this)."),(0,i.kt)("h2",{id:"image-regression"},"Image Regression"),(0,i.kt)("p",null,"Yes, X is images and y are floats.  Ex: key point model -> predicting location of something like the center of someone's face."),(0,i.kt)("h3",{id:"get-data"},"Get Data"),(0,i.kt)("p",null,"First step is to get data with ",(0,i.kt)("inlineCode",{parentName:"p"},"get_image_files")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# view the data and it's structure\npath = untar_data(URLs.BIWI_HEAD_POSE)\nPath.BASE_PATH = path\npath.ls().sorted()\n(path/'01').ls().sorted()\n\n# next get all the data systematically\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\n")),(0,i.kt)("p",null,"It is a good idea to see what you are working with as a general rule."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"You can inspect images with the following code"),(0,i.kt)("pre",{parentName:"admonition"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"im = PILImage.create(img_files[0])\nim.to_thumb(160)\n"))),(0,i.kt)("p",null,"Define the functions to extract the data you need from the files.  You can ignore what this does and treat it as a helper function, b/c your problem is likely to be specific."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"cal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n")),(0,i.kt)("h3",{id:"define-the-datablock"},"Define the DataBlock"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"biwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)),  \n)\n")),(0,i.kt)("p",null,"Note the splitter function: we want to ensure that our model can generalize to people that it hasn't seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person's images."),(0,i.kt)("admonition",{type:"important"},(0,i.kt)("p",{parentName:"admonition"},"Points and Data Augmentation: We're not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you're working with another library, you may need to disable data augmentation for these kinds of problems.")),(0,i.kt)("p",null,"The only other difference from the previous data block examples is that the second block is a ",(0,i.kt)("inlineCode",{parentName:"p"},"PointBlock"),". This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images"),(0,i.kt)("h3",{id:"debug-the-datablock"},"Debug the DataBlock"),(0,i.kt)("p",null,"Using ",(0,i.kt)("inlineCode",{parentName:"p"},"showbatch"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"dls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n")),(0,i.kt)("p",null,"Inspect the shape:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"xb,yb = dls.one_batch()\nxb.shape,yb.shape\n")),(0,i.kt)("h3",{id:"train-the-model"},"Train The Model"),(0,i.kt)("p",null,"As usual, we can use ",(0,i.kt)("inlineCode",{parentName:"p"},"cnn_learner")," to create our ",(0,i.kt)("inlineCode",{parentName:"p"},"Learner"),". Remember way back in <<chapter_intro>> how we used ",(0,i.kt)("inlineCode",{parentName:"p"},"y_range")," to tell fastai the range of our targets? We'll do the same here -  coordinates in fastai and PyTorch are always rescaled between -1 and +1 by the ",(0,i.kt)("inlineCode",{parentName:"p"},"PointBlock"),", which is why you pass (-1, 1) to ",(0,i.kt)("inlineCode",{parentName:"p"},"y_range"),"."),(0,i.kt)("h4",{id:"setting-y_range"},"Setting ",(0,i.kt)("inlineCode",{parentName:"h4"},"y_range")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Always use y_range when predicting a continous target\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n")),(0,i.kt)("p",null,"We didn't specify a loss function, which means we're getting whatever fastai chooses as the default."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},">>> dls.loss_func\nFlattenedLoss of MSELoss()\n")),(0,i.kt)("p",null,"Note also that we didn't specify any metrics. That's because the MSE is already a useful metric for this task (although it's probably more interpretable after we take the square root). "),(0,i.kt)("admonition",{type:"important"},(0,i.kt)("p",{parentName:"admonition"},"You should always set ",(0,i.kt)("inlineCode",{parentName:"p"},"y_range")," when predicting continuous targets.\n",(0,i.kt)("inlineCode",{parentName:"p"},"y_range")," is implemented in fastai using ",(0,i.kt)("inlineCode",{parentName:"p"},"sigmoid_range"),", which is defined as:"),(0,i.kt)("pre",{parentName:"admonition"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n")),(0,i.kt)("p",{parentName:"admonition"},"This is set as the final layer of the model, if ",(0,i.kt)("inlineCode",{parentName:"p"},"y_range")," is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range ",(0,i.kt)("inlineCode",{parentName:"p"},"(lo,hi)"),"."),(0,i.kt)("p",{parentName:"admonition"},"Here's what it looks like:"),(0,i.kt)("pre",{parentName:"admonition"},(0,i.kt)("code",{parentName:"pre",className:"language-python"},"plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n")),(0,i.kt)("p",{parentName:"admonition"},(0,i.kt)("img",{src:a(3735).Z,width:"802",height:"506"}))),(0,i.kt)("h4",{id:"find-the-learning-rate-and-then-train"},"Find the learning rate and then train"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn.lr_find()\nlr = 1e-2\nlearn.fine_tune(3, lr)\n")),(0,i.kt)("h4",{id:"inspect-the-results"},"Inspect the Results"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"learn.show_results(ds_idx=1, nrows=3, figsize=(6,8))\n")),(0,i.kt)("h2",{id:"loss-functions-1"},"Loss Functions"),(0,i.kt)("p",null,"fastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your ",(0,i.kt)("inlineCode",{parentName:"p"},"DataLoader"),"s, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"nn.CrossEntropyLoss")," for single-label classification"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"nn.BCEWithLogitsLoss")," for multi-label classification"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"nn.MSELoss")," for regression")))}u.isMDXComponent=!0},9314:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2022-03-05-13-18-01-fe546be7ea38a444f60e4ac6ea005d1b.png"},8876:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2022-03-06-13-46-46-ffd166d03e305e4a9c78c3e651b842a5.png"},3735:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2022-03-06-16-14-13-2bd05bac22f6583d71ad4a36147e262e.png"}}]);